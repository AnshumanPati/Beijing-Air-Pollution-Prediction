{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from itertools import chain\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     pollution  dew  temp   press wnd_dir  wnd_spd  snow  rain\n",
      "date                                                                          \n",
      "2010-01-02 00:00:00      129.0  -16  -4.0  1020.0      SE     1.79     0     0\n",
      "2010-01-02 01:00:00      148.0  -15  -4.0  1020.0      SE     2.68     0     0\n",
      "2010-01-02 02:00:00      159.0  -11  -5.0  1021.0      SE     3.57     0     0\n",
      "2010-01-02 03:00:00      181.0   -7  -5.0  1022.0      SE     5.36     1     0\n",
      "2010-01-02 04:00:00      138.0   -7  -5.0  1022.0      SE     6.25     2     0\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "# load data\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m %d %H')\n",
    "dataset = read_csv('raw.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n",
    "dataset.drop('No', axis=1, inplace=True)\n",
    "# manually specify column names\n",
    "dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
    "dataset.index.name = 'date'\n",
    "# mark all NA values with 0\n",
    "dataset['pollution'].fillna(0, inplace=True)\n",
    "# drop the first 24 hours\n",
    "dataset = dataset[24:]\n",
    "# summarize first 5 rows\n",
    "print(dataset.head(5))\n",
    "# save to file\n",
    "dataset.to_csv('pollution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     pollution  dew  temp   press wnd_dir  wnd_spd  snow  rain\n",
      "date                                                                          \n",
      "2010-01-02 00:00:00      129.0  -16  -4.0  1020.0      SE     1.79     0     0\n",
      "2010-01-02 01:00:00      148.0  -15  -4.0  1020.0      SE     2.68     0     0\n",
      "2010-01-02 02:00:00      159.0  -11  -5.0  1021.0      SE     3.57     0     0\n",
      "2010-01-02 03:00:00      181.0   -7  -5.0  1022.0      SE     5.36     1     0\n",
      "2010-01-02 04:00:00      138.0   -7  -5.0  1022.0      SE     6.25     2     0\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "# load data\n",
    "def parse(x):\n",
    "\treturn datetime.strptime(x, '%Y %m %d %H')\n",
    "dataset = read_csv('raw.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n",
    "dataset.drop('No', axis=1, inplace=True)\n",
    "# manually specify column names\n",
    "dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
    "dataset.index.name = 'date'\n",
    "# mark all NA values with 0\n",
    "dataset['pollution'].fillna(0, inplace=True)\n",
    "# drop the first 24 hours\n",
    "dataset = dataset[24:]\n",
    "# summarize first 5 rows\n",
    "print(dataset.head(5))\n",
    "# save to file\n",
    "dataset.to_csv('pollution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    chunk_size = 3\n",
    "    input_epochs = 500\n",
    "    input_batch_size = 100\n",
    "    url = r'/data/15demo.csv'\n",
    "    train_x, label_y, valid_x, valid_y, test_x, test_y, test_data, reframed = \\\n",
    "        get_train_valid_test_set(url=url, chunk_size_x=chunk_size)\n",
    "    mse_pre_src, correlation, spearman_correlation = lstm_model(url, train_x, label_y,\n",
    "                                                                valid_x, valid_y, test_x, test_y,\n",
    "                                                                input_epochs, input_batch_size,\n",
    "                                                                test_data, chunk_size_x=chunk_size)\n",
    "    \n",
    "    # print(reframed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_test_set(url, chunk_size_x):\n",
    "    data_frame = pd.read_csv(url)\n",
    "    data_set = data_frame.iloc[:, 1:2].values\n",
    "    data_set = data_set.astype('float64')\n",
    "\n",
    "    # sc = MinMaxScaler(feature_range=(0, 1))\n",
    "    # train_data_set = sc.fit_transform(data_set)\n",
    "\n",
    "    train_data_set = np.array(data_set)\n",
    "    reframed_train_data_set = np.array(series_to_supervised(train_data_set, chunk_size_x, 1).values)\n",
    "\n",
    "    train_days = int(len(reframed_train_data_set) * 0.6)\n",
    "    valid_days = int(len(reframed_train_data_set) * 0.2)\n",
    "\n",
    "    train = reframed_train_data_set[:train_days, :]\n",
    "    valid = reframed_train_data_set[train_days:train_days + valid_days, :]\n",
    "    test = reframed_train_data_set[train_days + valid_days:, :]\n",
    "\n",
    "    # test_data --- spearman correlation\n",
    "    test_data = train_data_set[train_days + valid_days + chunk_size_x:, :]\n",
    "\n",
    "    train_x, train_y = train[:, :-1], train[:, -1]\n",
    "    valid_x, valid_y = valid[:, :-1], valid[:, -1]\n",
    "    test_x, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "    train_x = train_x.reshape((train_x.shape[0], chunk_size_x, 1))\n",
    "    valid_x = valid_x.reshape((valid_x.shape[0], chunk_size_x, 1))\n",
    "    test_x = test_x.reshape((test_x.shape[0], chunk_size_x, 1))\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y, test_data, reframed_train_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_data_set(url, chunk_size_x):\n",
    "    data_frame = pd.read_csv(url)\n",
    "    data_set = data_frame.iloc[:, 1:2].values\n",
    "    data_set = data_set.astype('float64')\n",
    "\n",
    "    # sc = MinMaxScaler(feature_range=(0, 1))\n",
    "    # train_data_set = sc.fit_transform(data_set)\n",
    "\n",
    "    source_data_set = np.array(data_set)\n",
    "    # source_data_set = np.array(series_to_supervised(train_data_set, chunk_size_x, 1).values)\n",
    "    return source_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(url, train_x, label_y, valid_x, valid_y, test_x, test_y, input_epochs, input_batch_size, test_data,\n",
    "               chunk_size_x):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, activation='tanh', input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Trains the model for a given number of epochs (iterations on a dataset).\n",
    "    res = model.fit(train_x, label_y, epochs=input_epochs, batch_size=input_batch_size,\n",
    "                    validation_data=(valid_x, valid_y), verbose=2, shuffle=False)\n",
    "\n",
    "    # prediction Generates output predictions for the input samples.\n",
    "    train_predict = model.predict(train_x)\n",
    "    #valid_predict = model.predict(valid_x)\n",
    "    test_predict = model.predict(test_x)\n",
    "\n",
    "    test_data_list = list(chain(*test_data))\n",
    "    test_predict_list = list(chain(*test_predict))\n",
    "\n",
    "    # source_data_set = get_reframed_train_data_set(url=url, chunk_size_x=chunk_size_x)\n",
    "    source_data_set = get_source_data_set(url=url, chunk_size_x=chunk_size_x)\n",
    "\n",
    "    plt.plot(res.history['loss'], label='train')\n",
    "    plt.show()\n",
    "    print(model.summary())\n",
    "    plot_img(source_data_set, train_predict, valid_predict, test_predict)\n",
    "\n",
    "    correlation = get_correlation(test_data_list, test_predict_list)\n",
    "    spearman_correlation = get_spearman_correlation(test_data_list, test_predict_list)\n",
    "    mse_pre_src = get_mse_pre_src(test_data_list, test_predict_list)\n",
    "    return mse_pre_src, correlation, spearman_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse_pre_src(test_data, predict_data):\n",
    "    mse = mean_squared_error(test_data, predict_data)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def get_correlation(test_data, predict_data):\n",
    "    ans = np.corrcoef(np.array(test_data), np.array(predict_data))\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_spearman_correlation(test_data, predict_data):\n",
    "    df2 = pd.DataFrame({'real': test_data, 'prediction': predict_data})\n",
    "    return df2.corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(source_data_set, train_predict, valid_predict, test_predict):\n",
    "    plt.figure(figsize=(24, 8))\n",
    "    plt.plot(source_data_set[:, -1], c='b')\n",
    "    plt.plot([x for x in train_predict], c='g')\n",
    "    plt.plot([None for _ in train_predict] + [x for x in valid_predict], c='y')\n",
    "    plt.plot([None for _ in train_predict] + [None for _ in valid_predict] + [x for x in test_predict], c='r')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_mse_pre_src at 0x000001EE7ABE8D08>\n"
     ]
    }
   ],
   "source": [
    "print(get_mse_pre_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
